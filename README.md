# ðŸ§ª LLM Security Labs

Interactive labs that simulate realâ€‘world LLM vulnerabilities, mapped to the **OWASP Topâ€¯10 for LLMs**.

Each lab focuses on a different weaknessâ€”from classic promptâ€‘injection to modelâ€‘level backdoors.

---

## ðŸ“¦ Lab Matrix

| # | Lab | OWASP ID | Difficulty | Colab |
|---|------|----------|------------|-------|
| 01 | Prompt Injection | LLMâ€‘01 | ðŸŸ¢ Easy | [Colab](https://colab.research.google.com/github/codemedici/llm-security-labs/blob/main/labs/01_prompt_injection/prompt_injection_lab.ipynb) |
| 02 | Jailbreak Techniques | LLMâ€‘02 | ðŸŸ¡ Medium | [Colab](https://colab.research.google.com/github/codemedici/llm-security-labs/blob/main/labs/02_jailbreak/jailbreak_lab.ipynb) |
| 03 | Vector Store Poisoning | LLMâ€‘03/05 | ðŸŸ¡ Medium | *(coming)* |
| 04 | Insecure Output Filtering | LLMâ€‘04 | ðŸŸ¡ Medium | [Colab](https://colab.research.google.com/github/codemedici/llm-security-labs/blob/main/labs/04_insecure_output/firewall_bypass_lab.ipynb) |
| 05 | Data Extraction | LLMâ€‘04/06 | ðŸ”´ Hard | *(coming)* |
| 06 | Trainingâ€‘Data Poison | LLMâ€‘05 | ðŸ”´ Hard | *(coming)* |
| 07 | Adversarial Evasion | LLMâ€‘07 | ðŸŸ¡ Medium | *(coming)* |
| 08 | Insecure Plugins | LLMâ€‘08 | ðŸ”´ Hard | *(coming)* |
| 09 | Excessive Agency | LLMâ€‘09 | ðŸ”´ Hard | *(coming)* |
| 10 | Model Manipulation | LLMâ€‘10 | ðŸ”´ Hard | [Colab](https://colab.research.google.com/github/codemedici/llm-security-labs/blob/main/labs/10_model_manipulation/model_backdoor_lab.ipynb) |

**Legend:** ðŸŸ¢Â EasyÂ |Â ðŸŸ¡Â MediumÂ |Â ðŸ”´Â Hard

---

## ðŸš€ QuickÂ Start

```bash
# Clone & enter repo
$ git clone https://github.com/codemedici/llm-security-labs.git
$ cd llm-security-labs

# Launch lab 02 (Jailbreak)
$ make start-02_jailbreak
# â†’ http://localhost:8000
```

### Makefile targets

```makefile
# run vulnerable FastAPI bot for any lab
start-%:
	cd labs/$* && uvicorn vuln_$*_bot:app --reload

# (optional) pytest / nbval tests per lab
test-%:
	pytest tests/test_$*.py
```

---

## ðŸŽ® CTFÂ Mode

* Each lab contains a local `flag.txt`.
* Extracting the flag = completing the challenge.
* Optional scoreboard backend can record `/submit?flag=â€¦`.

---

## ðŸ›¡ Development & CI

A GitHub Actions workflow lints Python files with **Black** and executes every notebook using **nbval**.

```yaml
name: LabÂ CI
on:
  push:
    branches: [main]
  pull_request:

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: "3.10"
      - run: pip install black nbval pytest fastapi uvicorn sentence-transformers
      - run: black --check labs
      - run: |
          for nb in $(find labs -name "*.ipynb"); do
            jupyter nbconvert --execute "$nb" --to notebook --output /tmp/out.ipynb --ExecutePreprocessor.timeout=180
          done
```

Add this file as `.github/workflows/ci.yml`.

---

## ðŸ§ Â Credits

Part of the **LLMÂ Security Notebook** & DC4420 live demo.  
Maintainer: [@codemedici](https://github.com/codemedici) â€” MITÂ License.
